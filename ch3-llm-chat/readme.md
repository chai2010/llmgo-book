# 3. LLMèŠå¤©æœºå™¨äºº

LangChainæ˜¯ä¸“é—¨ç”¨äºå¼€å‘LLMé©±åŠ¨å‹åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œè€ŒLangChain Goæ˜¯LangChainæ¡†æ¶çš„Goè¯­è¨€å®ç°ã€‚ç°åœ¨æˆ‘ä»¬å°è¯•ç”¨LangChain Goè¿æ¥Ollamaè¿è¡Œçš„æœ¬åœ°å¤§æ¨¡å‹ï¼Œç„¶åæ„å»ºä¸€ä¸ªLLMèŠå¤©æœºå™¨äººã€‚

## 3.1 è¿æ¥Ollama

é€šè¿‡LangChainGoè¿æ¥Ollamaéå¸¸ç®€å•ï¼š

```go
package main

import (
	"context"
	"fmt"

	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/llms/ollama"
)

func main() {
	llm, _ := ollama.New(ollama.WithModel("deepseek-r1:1.5b"))
	completion, _ := llms.GenerateFromSinglePrompt(
		context.Background(), llm, "hello deepseek",
	)
	fmt.Println(completion)
}
```

æœ¬åœ°æ‰§è¡Œç»“æœå¦‚ä¸‹ï¼š

```
$ go run .
<think>

</think>

Hello! How can I assist you today? ğŸ˜Š
```

## 3.2 å‘½ä»¤è¡ŒèŠå¤©ç¨‹åº

å…ˆæ„é€ ä¸€ä¸ªå‘½ä»¤è¡Œç¯å¢ƒçš„èŠå¤©ç¨‹åºï¼Œæ¯æ¬¡å¯¹è¯ä¸åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é¦–å…ˆåŒ…è£…ä¸€ä¸ª`LLMChat`å‡½æ•°ï¼š


```go
func LLMChat(prompt string) (string, error) {
	llm, err := ollama.New(ollama.WithModel("deepseek-r1:1.5b"))
	if err != nil {
		return "", err
	}

	return llms.GenerateFromSinglePrompt(context.Background(), llm, prompt)
}
```

ç„¶åå†mainå‡½æ•°ä¸­ä»¥äº¤äº’çš„æ–¹å¼è°ƒç”¨`LLMChat`å‡½æ•°ï¼š

```go
func main() {
	// æç¤ºç”¨æˆ·
	fmt.Println("è¯·è¾“å…¥èŠå¤©å¯¹è¯å†…å®¹ï¼Œè¾“å…¥ '/bye' é€€å‡ºã€‚")

	// å¾ªç¯è¯»å–è¾“å…¥
	scanner := bufio.NewScanner(os.Stdin)
	for {
		// æç¤ºç”¨æˆ·è¾“å…¥
		fmt.Print(">> ")

		// è¯»å–ä¸€è¡Œè¾“å…¥
		scanner.Scan()
		input := scanner.Text()

		// å¦‚æœè¾“å…¥çš„æ˜¯ 'exit'ï¼Œé€€å‡ºå¾ªç¯
		if strings.ToLower(input) == "/bye" {
			break
		}

		response, err := LLMChat(input)
		if err != nil {
			fmt.Printf("ERROR: %v\n", err)
			continue
		}
		fmt.Println(response)
	}
}
```

æ‰§è¡Œçš„æ•ˆæœå¦‚ä¸‹ï¼š

```
$ go run .
è¯·è¾“å…¥èŠå¤©å¯¹è¯å†…å®¹ï¼Œè¾“å…¥ '/bye' é€€å‡ºã€‚
>> hello deepseek
<think>

</think>

Hello! How can I assist you today? ğŸ˜Š
>> /bye
$
```

